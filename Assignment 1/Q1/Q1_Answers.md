# Assignment 1 - Q1 答案文档

## 问题1：Transformer模型的核心组件

### 1.1 Multi-Head Attention机制

**问题：** 解释Multi-Head Attention的工作原理和优势。

**答案：**

Multi-Head Attention是Transformer模型的核心组件，它允许模型同时关注输入序列的不同位置和不同表示子空间。

**工作原理：**
1. **线性变换：** 输入通过三个线性层分别生成Query(Q)、Key(K)、Value(V)矩阵
2. **多头分割：** 将embedding维度分割成多个头，每个头独立计算注意力
3. **注意力计算：** 对每个头计算 `Attention(Q,K,V) = softmax(QK^T/√d_k)V`
4. **拼接输出：** 将所有头的输出拼接并通过线性层

**优势：**
- **并行处理：** 多个头可以并行计算，提高效率
- **多样化表示：** 不同头可以学习不同类型的依赖关系
- **增强表达能力：** 相比单头注意力，多头机制能捕获更复杂的模式

### 1.2 Positional Encoding

**问题：** 为什么需要位置编码？如何实现？

**答案：**

**为什么需要位置编码：**
- Transformer使用自注意力机制，本身不具备位置信息
- 词序对语言理解至关重要
- 需要显式地告诉模型每个词的位置

**实现方法：**
使用正弦和余弦函数生成位置编码：
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**优势：**
- 能够表示任意长度的序列
- 相对位置关系可以被学习
- 对训练和推理都很友好

### 1.3 Layer Normalization

**问题：** Layer Normalization的作用和实现。

**答案：**

**作用：**
- **稳定训练：** 减少内部协变量偏移
- **加速收敛：** 使梯度更稳定
- **提高泛化能力：** 减少对初始化的敏感性

**实现：**
```python
def layer_norm(x, gamma, beta, eps=1e-6):
    mean = x.mean(dim=-1, keepdim=True)
    std = x.std(dim=-1, keepdim=True)
    return gamma * (x - mean) / (std + eps) + beta
```

**与Batch Normalization的区别：**
- Layer Norm：在特征维度上归一化
- Batch Norm：在批次维度上归一化
- Layer Norm更适合序列数据

## 问题2：Transformer架构分析

### 2.1 Encoder-Decoder结构

**问题：** 解释Encoder和Decoder的作用和区别。

**答案：**

**Encoder：**
- **作用：** 将输入序列编码为上下文表示
- **结构：** 多层自注意力 + 前馈网络
- **特点：** 可以并行处理整个输入序列

**Decoder：**
- **作用：** 基于编码器输出生成目标序列
- **结构：** 自注意力 + 交叉注意力 + 前馈网络
- **特点：** 需要掩码机制防止看到未来信息

**关键区别：**
1. **注意力机制：** Decoder有额外的交叉注意力层
2. **掩码：** Decoder需要因果掩码
3. **处理方式：** Encoder并行，Decoder自回归

### 2.2 残差连接和Dropout

**问题：** 残差连接和Dropout的作用。

**答案：**

**残差连接：**
- **作用：** 解决梯度消失问题，使深层网络更容易训练
- **实现：** `output = LayerNorm(sublayer(x) + x)`
- **优势：** 允许梯度直接传播，提高训练稳定性

**Dropout：**
- **作用：** 防止过拟合，提高模型泛化能力
- **位置：** 在注意力权重和FFN输出后应用
- **效果：** 随机将部分神经元置零，增加模型鲁棒性

## 问题3：实现细节

### 3.1 掩码机制

**问题：** 解释不同类型的掩码及其用途。

**答案：**

**Padding Mask：**
- **用途：** 忽略填充位置的注意力
- **实现：** 将填充位置设为-∞
- **作用：** 确保模型不关注无意义的填充token

**Causal Mask：**
- **用途：** 防止解码器看到未来信息
- **实现：** 上三角矩阵，上三角部分设为-∞
- **作用：** 保证自回归生成的一致性

### 3.2 前馈网络

**问题：** FeedForward网络的设计原理。

**答案：**

**设计：**
```python
FFN(x) = max(0, xW1 + b1)W2 + b2
```

**特点：**
- **两层线性变换：** 先升维再降维
- **ReLU激活：** 增加非线性
- **Dropout：** 防止过拟合

**作用：**
- 为每个位置提供独立的处理
- 增加模型的表达能力
- 与注意力机制互补

## 问题4：训练和优化

### 4.1 参数初始化

**问题：** Xavier初始化的原理。

**答案：**

**原理：**
- 保持前向和反向传播时激活值和梯度的方差
- 对于均匀分布：`U(-√(6/(fan_in + fan_out)), √(6/(fan_in + fan_out)))`
- 对于正态分布：`N(0, √(2/(fan_in + fan_out)))`

**优势：**
- 防止梯度爆炸和消失
- 加速收敛
- 特别适合sigmoid和tanh激活函数

### 4.2 学习率调度

**问题：** Transformer中常用的学习率调度策略。

**答案：**

**Warmup + Decay：**
```python
lr = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))
```

**优势：**
- **Warmup：** 避免训练初期的不稳定
- **Decay：** 后期精细调优
- **自适应：** 根据模型大小调整

## 总结

Transformer模型通过以下创新实现了突破性进展：

1. **并行化训练：** 相比RNN，可以并行处理序列
2. **长距离依赖：** 自注意力机制直接建模任意距离的依赖
3. **可解释性：** 注意力权重提供模型决策的可视化
4. **可扩展性：** 架构简单，易于扩展和改进

这些特性使得Transformer成为现代NLP的基础架构，为后续的预训练模型（如BERT、GPT）奠定了基础。
